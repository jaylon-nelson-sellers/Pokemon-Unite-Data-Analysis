{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4966a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "import os\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a491e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and resize image\n",
    "def get_image(path, zoom=0.3):\n",
    "    return OffsetImage(plt.imread(path), zoom=zoom)\n",
    "def preprocess_data():\n",
    "    # Load spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Function to lemmatize text using spaCy\n",
    "    def lemmatize_text(text):\n",
    "        doc = nlp(text)\n",
    "        return \" \".join([token.lemma_ for token in doc])\n",
    "    \n",
    "\n",
    "    # Load the data\n",
    "    data = pd.read_csv('raw_data.csv')\n",
    "\n",
    "    # Extract character names and remove from main data\n",
    "    character_names = data.iloc[:, 0].tolist()\n",
    "    main_data = data.iloc[:, 1:]\n",
    "\n",
    "    # Identify numeric and text columns\n",
    "    numeric_columns = main_data.select_dtypes(include=[np.number]).columns\n",
    "    text_columns = main_data.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    # Preprocess numeric columns\n",
    "    scaler = StandardScaler()\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    numeric_data = pd.DataFrame(scaler.fit_transform(imputer.fit_transform(main_data[numeric_columns])), \n",
    "                                columns=numeric_columns)\n",
    "\n",
    "    print(main_data[text_columns].shape)\n",
    "\n",
    "    # Preprocess text columns using TfidfVectorizer and spaCy lemmatization\n",
    "    tfidf = TfidfVectorizer()  # You can adjust max_features as needed\n",
    "    text_data = main_data[text_columns].fillna('')\n",
    "    text_data_combined = text_data.apply(lambda x: ' '.join(x), axis=1)\n",
    "    lemmatized_text = text_data_combined.apply(lemmatize_text)\n",
    "    tfidf_matrix = tfidf.fit_transform(lemmatized_text)\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "    # Combine processed data\n",
    "    processed_data = pd.concat([numeric_data,tfidf_df], axis=1)\n",
    "    return [processed_data,character_names]\n",
    "\n",
    "def decompose_data(processed_data,character_names):\n",
    "    # Apply TSNE\n",
    "    processed_data\n",
    "\n",
    "    tsne = PCA(n_components=1)\n",
    "    tsne_1d = tsne.fit_transform(processed_data)\n",
    "    tsne_1d = pd.DataFrame(tsne_1d, columns=['x'])\n",
    "    tsne_1d.insert(loc = 0,\n",
    "          column = \"Character\",\n",
    "          value = character_names)\n",
    "    tsne_1d.to_csv(\"data-1d.csv\",index=False)\n",
    "\n",
    "    tsne2 = PCA(n_components=2)\n",
    "    tsne_2d = tsne2.fit_transform(processed_data)\n",
    "    tsne_2d = pd.DataFrame(tsne_2d, columns=['x', 'y'])\n",
    "    tsne_2d.insert(loc = 0,\n",
    "          column = \"Character\",\n",
    "          value = character_names)\n",
    "    tsne_2d.to_csv(\"data-2d.csv\",index=False)\n",
    "\n",
    "    tsne3 = KernelPCA(n_components=3)\n",
    "    tsne_3d = tsne3.fit_transform(processed_data)\n",
    "    tsne_3d = pd.DataFrame(tsne_3d, columns=['x', 'y', 'z'])\n",
    "    tsne_3d.insert(loc = 0,\n",
    "          column = \"Character\",\n",
    "          value = character_names)\n",
    "    tsne_3d.to_csv(\"data-3d.csv\",index=False)\n",
    "\n",
    "    # Create a graph with images\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    scatter = plt.scatter(tsne_2d['x'], tsne_2d['y'], alpha=0)\n",
    "\n",
    "    for i, character in enumerate(tsne_2d['Character']):\n",
    "        try:\n",
    "            img_path = os.path.join('pokemon_images', f\"{character.lower().replace(' ', '_')}.png\")\n",
    "            ab = AnnotationBbox(get_image(img_path), (tsne_2d['x'][i], tsne_2d['y'][i]), frameon=False)\n",
    "            plt.gca().add_artist(ab)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Image not found for {character}\")\n",
    "\n",
    "    plt.title('KPCA Clustering of Pokemon Characters', fontsize=36)\n",
    "    plt.xlabel('Component 1', fontsize=36)\n",
    "    plt.ylabel('Component 2', fontsize=36)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('KPCA_clustering_images.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Processing complete. Results saved to 'clustered_main_data.csv' and 'TSNE_clustering_images.png'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6beada91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68, 22)\n",
      "Processing complete. Results saved to 'clustered_main_data.csv' and 'TSNE_clustering_images.png'.\n"
     ]
    }
   ],
   "source": [
    "df,chars = preprocess_data()\n",
    "decompose_data(df,chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b66e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaylo\\AppData\\Local\\Temp\\ipykernel_76020\\2368278976.py:28: UserWarning: color is redundantly defined by the 'color' keyword argument and the fmt string \"bo-\" (-> color='b'). The keyword argument will take precedence.\n",
      "  plt.plot(cluster_range, inertia, 'bo-', color='b')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster analysis complete. Results saved to 1d_clustered_output.csv and Elbow graph to 1d_elbow_graph.png.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "def perform_cluster_analysis(df, output_csv_path, elbow_graph_path, max_clusters=10):\n",
    "    df = df.fillna(-1)\n",
    "    # Step 1: Separate labels (assumed to be in the first column)\n",
    "    labels = df.iloc[:, 0]\n",
    "    data = df.iloc[:, 1:]\n",
    "\n",
    "    # Step 2: Standardize the data (since KMeans is sensitive to scale)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "    # Step 3: Perform the Elbow Method to find the optimal number of clusters\n",
    "    inertia = []\n",
    "    cluster_range = range(1, max_clusters+1)\n",
    "    \n",
    "    for k in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(scaled_data)\n",
    "        inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    # Step 4: Plot the Elbow graph and save it\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(cluster_range, inertia, 'bo-', color='b')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Elbow Method for Optimal k')\n",
    "    plt.savefig(elbow_graph_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Step 5: Choose the optimal number of clusters manually (for now, based on the Elbow graph)\n",
    "    # Alternatively, you could implement automatic elbow detection with additional logic\n",
    "    optimal_k = int(input(\"Enter the optimal number of clusters based on the elbow graph: \"))\n",
    "\n",
    "    # Step 6: Perform KMeans clustering with the chosen number of clusters\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    clusters = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "    df = df.fillna(-1)\n",
    "    # Step 7: Create a new dataframe to hold the labels and cluster assignments\n",
    "    df['clusters'] = clusters\n",
    "\n",
    "    # Step 8: Save the new dataframe with cluster assignments to CSV\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Cluster analysis complete. Results saved to {output_csv_path} and Elbow graph to {elbow_graph_path}.\")\n",
    "\n",
    "# Test Raw Data\n",
    "#df = pd.read_csv('raw_data.csv')\n",
    "#perform_cluster_analysis(df, 'raw_clustered_output.csv', 'raw_elbow_graph.png')\n",
    "# Test 1-D Data\n",
    "df = pd.read_csv('data-1d.csv')\n",
    "perform_cluster_analysis(df, '1d_clustered_output.csv', '1d_elbow_graph.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
